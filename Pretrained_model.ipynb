{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TioqwgOxV0Ez"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AeqCB0xeV5RI"
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(\"chap4_mahabharath.csv\", index_col=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "n0-o2xPCWEzv",
    "outputId": "e3e0c26c-56fe-45e3-f3da-3e41387a87a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence#</th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Om</td>\n",
       "      <td>CONCEPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Having</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>bowed</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>down</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence#   token      tag\n",
       "id                            \n",
       "0           0      Om  CONCEPT\n",
       "1           0       !        O\n",
       "2           1  Having        O\n",
       "3           1   bowed        O\n",
       "4           1    down        O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPMawT24WEzG",
    "outputId": "9f3c8987-d028-4fcb-e3f7-c192dc869e6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19573, 830516)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(annotations['token'].values)\n",
    "words.append('PADword')\n",
    "n_words = len(set(words))\n",
    "n_words, len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edTLfTmDWEsV",
    "outputId": "7062efbd-bfd8-41b0-ad4c-b2d7aa51d34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PLANT',\n",
       " 'BOOK',\n",
       " 'WATER',\n",
       " 'CONCEPT',\n",
       " 'PLACE',\n",
       " 'O',\n",
       " 'ANIMAL',\n",
       " 'GROUP',\n",
       " 'SPECIAL_OBJECT',\n",
       " 'WEAPON',\n",
       " 'TITLE',\n",
       " 'PERSON']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(annotations[\"tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(n_tags)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uUNaJRi1WEpr"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"token\"].values.tolist(),s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence#\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9TiFXg9WD5M",
    "outputId": "6471f50d-ef4a-482e-a153-db4be9a88816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Om', 'CONCEPT'), ('!', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(annotations)\n",
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrKv5dhDWDiG",
    "outputId": "d5c16f70-177d-47a5-dad8-03ac7ec46738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34332\n"
     ]
    }
   ],
   "source": [
    "sentences = getter.sentences\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iJ0ajrfPWDce",
    "outputId": "f96f72f5-7e80-4bb3-c722-8e21e4a2fd38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONCEPT'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfG8RTW7WDZl",
    "outputId": "a94aa574-f0d5-43fd-d345-bd5a3c972e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biggest sentence has 428 words\n"
     ]
    }
   ],
   "source": [
    "largest_sen = max(len(sen) for sen in sentences)\n",
    "print('biggest sentence has {} words'.format(largest_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "0CusPfViWDXM",
    "outputId": "7873cd18-a50a-4b43-83f3-9d0525a80f3b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) for s in sentences], bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Om', 'CONCEPT'), ('!', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eABhhmKhWDUs"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X = [[str(w[0]) for w in s] for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Om', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZsEus-axWf5B"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "living_entity_tags = ['ANIMAL','PERSON','GROUP','TITLE']\n",
    "non_living_entity_tags = ['BOOK','PLACE','WEAPON','SPECIAL_OBJECT','PLANT','CONCEPT','WATER']\n",
    "\n",
    "#for extraction of entities\n",
    "tags2index = {}\n",
    "for tag in tags:\n",
    "    if tag not in living_entity_tags and tag not in non_living_entity_tags:\n",
    "        tags2index[tag] = 0\n",
    "    elif tag in living_entity_tags:\n",
    "        tags2index[tag] = 1\n",
    "    else:\n",
    "        tags2index[tag] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [[tags2index[w[1]] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Having',\n",
       "  'bowed',\n",
       "  'down',\n",
       "  'to',\n",
       "  'Narayana',\n",
       "  'and',\n",
       "  'Nara',\n",
       "  ',',\n",
       "  'the',\n",
       "  'most',\n",
       "  'exalted',\n",
       "  'male',\n",
       "  'being',\n",
       "  ',',\n",
       "  'and',\n",
       "  'also',\n",
       "  'to',\n",
       "  'the',\n",
       "  'goddess',\n",
       "  'Saraswati',\n",
       "  ',',\n",
       "  'must',\n",
       "  'the',\n",
       "  'word',\n",
       "  'Jaya',\n",
       "  'be',\n",
       "  'uttered',\n",
       "  '.'],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1], Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sV9GtxSVWf2U"
   },
   "outputs": [],
   "source": [
    "Y = pad_sequences(maxlen=max_len, sequences=Y, value=0, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FffpIlhVWf0H",
    "outputId": "8030bfdf-dea8-4721-c004-92ad537af78f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zsKmMitBWfxU"
   },
   "outputs": [],
   "source": [
    "X_sent=[]\n",
    "for i in range(len(X)):\n",
    "    X_sent.append(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Om', '!']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AoPLZf6NWfu7"
   },
   "outputs": [],
   "source": [
    "X_join=[]\n",
    "for i in range(len(X_sent)):\n",
    "    X_join.append(\" \".join(X_sent[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_231_dqWfsg",
    "outputId": "af4adfa8-7822-4546-8356-282c4c0eed4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34332, 'Om !')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_join), X_join[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zzI1Ch6tWfnD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PLANT': 2,\n",
       " 'BOOK': 2,\n",
       " 'WATER': 2,\n",
       " 'CONCEPT': 2,\n",
       " 'PLACE': 2,\n",
       " 'O': 0,\n",
       " 'ANIMAL': 1,\n",
       " 'GROUP': 1,\n",
       " 'SPECIAL_OBJECT': 2,\n",
       " 'WEAPON': 2,\n",
       " 'TITLE': 1,\n",
       " 'PERSON': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gkhEVieCWfkM"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import add, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda,concatenate\n",
    "import seqeval\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IF-DsI8NXDF9"
   },
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X_join, Y, test_size=0.1, random_state=2021,shuffle = True)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dZWfRKpGXDDs"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert = 'farhanjafri/my-model'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=max_len, paddind=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_rEvmW6HXDBI"
   },
   "outputs": [],
   "source": [
    " def tokenize(sentences, tokenizer):\n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    #input_segments = []\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer.encode_plus(sent, \n",
    "                                     add_special_tokens=True,\n",
    "                                     max_length=50,\n",
    "                                     pad_to_max_length = True, \n",
    "                                     return_attention_mask=True,\n",
    "                                     return_token_type_ids=True,\n",
    "                                     truncation=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        #input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTeLEPYWXC-8",
    "outputId": "e9145da8-f07d-4360-e5ca-0991deeffed3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30898, 3434, 32, 965.5625, 107.3125)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tr), len(X_te), batch_size,len(X_tr)/batch_size, len(X_te)/batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "RP48Hl-4XC95"
   },
   "outputs": [],
   "source": [
    "X_tr, X_val = X_tr[:865*batch_size], X_tr[-100*batch_size:]\n",
    "y_tr, y_val = y_tr[:865*batch_size], y_tr[-100*batch_size:]\n",
    "#y_tr = y_tr.reshape(y_tr.shape[0], 1)\n",
    "#y_val = y_val.reshape(y_val.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZz9m1cCXC5m",
    "outputId": "4f84918e-fc04-4486-d6c1-aa70b6e0bccb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_tr_in,X_tr_mask= tokenize(X_tr,tokenizer)\n",
    "X_val_in,X_val_mask = tokenize(X_val,tokenizer)\n",
    "X_te_in,X_te_mask = tokenize(X_te,tokenizer)\n",
    "#y_tr_in,y_tr_mask,_ = np.array(tokenize(y_tr,tokenizer))\n",
    "#y_val_in,y_val_mask,_ = np.array(tokenize(y_val,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxffJ5ZKXC3N",
    "outputId": "aecb4d64-3a5b-4401-80cd-355b40a06b72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  8254,  2072, ...,     0,     0,     0],\n",
       "       [  101,  1998,  2588, ...,     0,     0,     0],\n",
       "       [  101,  1998, 15544, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1045,  2572, ...,     0,     0,     0],\n",
       "       [  101,  1998,  2383, ...,     0,     0,     0],\n",
       "       [  101,  1011,  1011, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWBWXmb6XC0t",
    "outputId": "d57bbf8d-536e-4bd0-a8a8-b39fcfea748e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27680, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_tr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asvN0fkrIV39",
    "outputId": "ec9b147b-3e10-4b21-87db-1fb011dbb15f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at farhanjafri/my-model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "transformer_model =TFBertForPreTraining.from_pretrained('farhanjafri/my-model',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hancaSNLXQ6p",
    "outputId": "562a26f5-f718-4122-96bf-ccc489ddbc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertForPreTraining.call of <transformers.models.bert.modeling_tf_bert.TFBertForPreTraining object at 0x7f2c019a6640>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertForPreTraining.call of <transformers.models.bert.modeling_tf_bert.TFBertForPreTraining object at 0x7f2c019a6640>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7f2c019872b0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7f2c019872b0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7f2c01987520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7f2c01987520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7f2c0197c6a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7f2c0197c6a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7f2c0197c070>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7f2c0197c070>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7f2c5d291100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7f2c5d291100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7f2c5cd9c250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7f2c5cd9c250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c019a6280>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c019a6280>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.models.bert.modeling_tf_bert.TFBertIntermediate object at 0x7f2c019ad6d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.models.bert.modeling_tf_bert.TFBertIntermediate object at 0x7f2c019ad6d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertOutput object at 0x7f2c019ada90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertOutput object at 0x7f2c019ada90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7f2c0197c9a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7f2c0197c9a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertMLMHead.call of <transformers.models.bert.modeling_tf_bert.TFBertMLMHead object at 0x7f2c00392d90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMLMHead.call of <transformers.models.bert.modeling_tf_bert.TFBertMLMHead object at 0x7f2c00392d90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertLMPredictionHead.call of <transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead object at 0x7f2c003580a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLMPredictionHead.call of <transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead object at 0x7f2c003580a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPredictionHeadTransform.call of <transformers.models.bert.modeling_tf_bert.TFBertPredictionHeadTransform object at 0x7f2c003581f0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPredictionHeadTransform.call of <transformers.models.bert.modeling_tf_bert.TFBertPredictionHeadTransform object at 0x7f2c003581f0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertNSPHead.call of <transformers.models.bert.modeling_tf_bert.TFBertNSPHead object at 0x7f2c00392820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertNSPHead.call of <transformers.models.bert.modeling_tf_bert.TFBertNSPHead object at 0x7f2c00392820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_pre_training (TFBer TFBertForPreTraining 110106428   input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 1572)     196870992   tf_bert_for_pre_training[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 1572)     14833392    bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 50, 1572)     0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 2056)     3234088     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50, 3)        6171        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 325,051,071\n",
      "Trainable params: 325,051,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoModel\n",
    "#transformer_model =AutoModel.from_pretrained('farhanjafri/my-model')\n",
    "\n",
    "input_ids_in=tf.keras.layers.Input(shape=(max_len,),name='input_token',dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(max_len,),name='masked_token',dtype='int32')\n",
    "#input_segments_in = tf.keras.layers.Input(shape=(max_len,),name='segment_token',dtype='int32')\n",
    "\n",
    "embedding_layer = transformer_model([input_ids_in,input_masks_in])[0]\n",
    "#cls_token = embedding_layer[:,:]\n",
    "#dense = Dense(786,activation = 'relu')(cls_token)\n",
    "x = Bidirectional(LSTM(units=786, return_sequences=True,\n",
    "                      recurrent_dropout=0.2, dropout=0.2))(embedding_layer)\n",
    "x_rnn = Bidirectional(LSTM(units=786, return_sequences=True,\n",
    "                          recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  \n",
    "x = Dense(2056,activation='relu')(x)\n",
    "#out = Dense(1024, activation=\"relu\")(cls_token)\n",
    "out = Dense(3, activation=\"softmax\")(x)\n",
    "model = Model([input_ids_in,input_masks_in], out)\n",
    "\n",
    "#for layer in model.layers[:3]:\n",
    "     #layer.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "esJ8l3HNXQ5x"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqA2AKMxXQwZ",
    "outputId": "8fa83ca9-09ac-423b-efdf-c58d32c1b42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: livelossplot in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (0.5.4)\n",
      "Requirement already satisfied: bokeh in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from livelossplot) (2.3.3)\n",
      "Requirement already satisfied: ipython in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from livelossplot) (7.25.0)\n",
      "Requirement already satisfied: matplotlib in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from livelossplot) (3.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from bokeh->livelossplot) (2.8.2)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from bokeh->livelossplot) (3.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/shreya15096/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (5.4.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/shreya15096/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/shreya15096/.local/lib/python3.9/site-packages (from bokeh->livelossplot) (3.7.4.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from bokeh->livelossplot) (6.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from bokeh->livelossplot) (1.19.5)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from bokeh->livelossplot) (8.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/shreya15096/.local/lib/python3.9/site-packages (from packaging>=16.8->bokeh->livelossplot) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/shreya15096/.local/lib/python3.9/site-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (3.0.17)\n",
      "Requirement already satisfied: matplotlib-inline in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (0.1.2)\n",
      "Requirement already satisfied: pickleshare in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: pygments in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (2.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (0.18.0)\n",
      "Requirement already satisfied: backcall in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (5.0.9)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from ipython->livelossplot) (5.0.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from jedi>=0.16->ipython->livelossplot) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from pexpect>4.3->ipython->livelossplot) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from matplotlib->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages (from matplotlib->livelossplot) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "rkDhS33nXQtw"
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "#checkpoints = ModelCheckpoint('checkpoints.h5', monitor='loss', save_best_only=True, verbose=1, mode='min')\n",
    "#early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "\n",
    "\n",
    "# callbacks = [PlotLossesKeras()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "F5HsPIFkXQrh"
   },
   "outputs": [],
   "source": [
    "X_tr_in = tf.convert_to_tensor(X_tr_in)\n",
    "X_tr_mask = tf.convert_to_tensor(X_tr_mask)\n",
    "#X_tr_seg = tf.convert_to_tensor(X_tr_seg)\n",
    "X_val_in = tf.convert_to_tensor(X_val_in)\n",
    "X_val_mask = tf.convert_to_tensor(X_val_mask)\n",
    "#X_val_seg = tf.convert_to_tensor(X_val_seg)\n",
    "y_tr = tf.convert_to_tensor(y_tr)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEbAkGeeXQpI",
    "outputId": "d9e86318-92d1-4ac9-9a8e-df437611cbc2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "\ttraining         \t (min:    0.972, max:    0.972, cur:    0.972)\n",
      "\tvalidation       \t (min:    0.966, max:    0.966, cur:    0.966)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.756, max:    0.756, cur:    0.756)\n",
      "\tvalidation       \t (min:    1.049, max:    1.049, cur:    1.049)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fddf005d1f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit([X_tr_in,X_tr_mask],y_tr,\n",
    "#                     validation_data=([X_val_in,X_val_mask],y_val),\n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=1,\n",
    "#                     callbacks=[callbacks], \n",
    "#                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7zEJPBo_XQmy"
   },
   "outputs": [],
   "source": [
    "X_te = X_te[:107*batch_size]\n",
    "y_te = y_te[:107*batch_size]\n",
    "X_te_in,X_te_mask = np.array(tokenize(X_te,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"model.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "gsJ9K3dvXQkd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f2bdc7a9e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f2bdc7a9e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "107/107 [==============================] - 739s 7s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([X_te_in,X_te_mask], verbose=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865/865 [==============================] - 5955s 7s/step\n"
     ]
    }
   ],
   "source": [
    "tr_preds = model.predict([X_tr_in,X_tr_mask], verbose=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Siniwali was the third daughter of Angiras .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "e,e1 = tokenize(\"Shreya is here\",tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27680, 50), dtype=int32, numpy=\n",
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
       "array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9806189 , 0.01805369, 0.00132741], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "kl9wt2f0XQh3"
   },
   "outputs": [],
   "source": [
    "# in case of classification\n",
    "# idx2tag = {i: w for w, i in tags2index.items()}\n",
    "\n",
    "# in case of extraction of entities\n",
    "idx2tag = {}\n",
    "for k,v in tags2index.items():\n",
    "    if v == 0:\n",
    "        idx2tag[v] = 'O'\n",
    "    elif v == 1:\n",
    "        idx2tag[v] = \"LIVING\"\n",
    "    else:\n",
    "        idx2tag[v] = \"NON-LIVING\"\n",
    "        \n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "def test2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "H9PGXgu4Xt00"
   },
   "outputs": [],
   "source": [
    "pred_labels = pred2label(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "_APMWQTcXtz5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "pTQA4DHkXtpa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3424, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "m9d6pnU5X2lh"
   },
   "outputs": [],
   "source": [
    "test_labels = test2label(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8A-2WWGqX2iu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3424, 50)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "NfAra2R9X2f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LIVING seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NON-LIVING seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples:\n[50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-af08e8e4cbde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, digits, suffix, output_dict, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;31m# compute per-class scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     p, r, f1, s = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, suffix)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     precision, recall, f_score, true_sum = _precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36m_precision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, scheme, suffix, extract_tp_actual_correct)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'average has to be one of {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mpred_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shreya15096/.conda/envs/virtualEnv/lib/python3.9/site-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen_true\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Found input variables with inconsistent numbers of samples:\\n{}\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples:\n[50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_labels, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t70GLqXfX2aH"
   },
   "outputs": [],
   "source": [
    "print(f1_score(pred_labels, test_labels,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled30.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
